{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from evaluation import evaluate_rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 entries...\n",
      "38 entries are used for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE-1': 0.70833, 'ROUGE-2': 0.28064, 'ROUGE-SU4': 0.38226}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_file = 'extract_output/testsvr_output.txt'\n",
    "evaluate_rouge_scores(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 entries...\n",
      "38 entries are used for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE-1': 0.7617, 'ROUGE-2': 0.3287, 'ROUGE-SU4': 0.4157}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_file = 'extract_output/testorg_output.txt'\n",
    "evaluate_rouge_scores(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 entries...\n",
      "334 entries are used for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE-1': 0.76465, 'ROUGE-2': 0.37318, 'ROUGE-SU4': 0.43912}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_file = 'extract_output/trainsvr_output.txt'\n",
    "evaluate_rouge_scores(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 entries...\n",
      "47 entries are used for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE-1': 0.77034, 'ROUGE-2': 0.37549, 'ROUGE-SU4': 0.45993}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_file = 'extract_output/devsvr_output.txt'\n",
    "evaluate_rouge_scores(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 entries...\n",
      "334 entries are used for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE-1': 0.79921, 'ROUGE-2': 0.4081, 'ROUGE-SU4': 0.47271}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_file = 'extract_output/trainorg_output.txt'\n",
    "evaluate_rouge_scores(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 entries...\n",
      "47 entries are used for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE-1': 0.85438, 'ROUGE-2': 0.42065, 'ROUGE-SU4': 0.50777}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_file = 'extract_output/devorg_output.txt'\n",
    "evaluate_rouge_scores(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 entries...\n",
      "334 entries are used for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE-1': 0.7376, 'ROUGE-2': 0.35127, 'ROUGE-SU4': 0.41934}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_file = 'org_text/train_rouge.txt'\n",
    "evaluate_rouge_scores(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 entries...\n",
      "47 entries are used for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE-1': 0.78392, 'ROUGE-2': 0.38775, 'ROUGE-SU4': 0.46705}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_file = 'org_text/dev_rouge.txt'\n",
    "evaluate_rouge_scores(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 entries...\n",
      "38 entries are used for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE-1': 0.71024, 'ROUGE-2': 0.27204, 'ROUGE-SU4': 0.37444}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_file = 'org_text/test_rouge.txt'\n",
    "evaluate_rouge_scores(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pythonrouge.pythonrouge import Pythonrouge\n",
    "def evaluate_rouge_scores(evaluation_file):\n",
    "    summaries = [] # model-generated\n",
    "    references = [] # human-generated\n",
    "    # articles = {}\n",
    "    with open(evaluation_file, encoding='utf8') as file:\n",
    "        evaluation_lines = file.read().strip().split('\\n')\n",
    "        print(\"%d entries...\" % len(evaluation_lines))\n",
    "        for line in evaluation_lines:\n",
    "            sum_line = line.split('\\t')[0]\n",
    "            ref_line = line.split('\\t')[1:]\n",
    "            summaries.append( sum_line.encode('utf-8').split(b\"</s><s>\"))\n",
    "            references.append([ example.encode('utf-8').split(b\"</s><s>\") for example in ref_line])\n",
    "    print(\"%d entries are used for evaluation.\" % len(summaries))\n",
    "    \n",
    "    rouge = Pythonrouge(summary_file_exist=False,\n",
    "                    summary=summaries, reference=references,\n",
    "                    n_gram=2, ROUGE_SU4=True, ROUGE_L=False,\n",
    "                    recall_only=True, stemming=True, stopwords=True,\n",
    "                    word_level=True, length_limit=True, length=50,\n",
    "                    use_cf=False, cf=95, scoring_formula='best',\n",
    "                    resampling=True, samples=1000, favor=True, p=0.5)\n",
    "    score = rouge.calc_score()\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 entries...\n",
      "334 entries are used for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE-1': 0.7376, 'ROUGE-2': 0.35127, 'ROUGE-SU4': 0.41934}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_file = 'org_text/train_rouge.txt'\n",
    "evaluate_rouge_scores(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 entries...\n",
      "334 entries are used for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE-1': 0.75749, 'ROUGE-2': 0.36966, 'ROUGE-SU4': 0.43502}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_file = 'extract_output/trainsvr_output.txt'\n",
    "evaluate_rouge_scores(res_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
